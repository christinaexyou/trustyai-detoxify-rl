# TrustyAI Detoxify
Detoxifying large language models is challenging. Reinforcement learning with human feedback is the standard practice for aligning LLMs with human values. It usually consists of the following steps 1) supervised finetuning 2) reward modeling 3) reinforcement learning. The first two steps can be major obstacles for data scientists and/or ML engineers because they require the curation of high quality data to train separate models.

We aim to overcome them by using TrustyAI Detoxify to filter for nontoxic prompts to train our model during supervised finetuning and to provide a reward signal to further refine it during reinforcement learning.

## Reinforcement Learning with Human Feedback (RHLF) & Its Challenges
Given an agent (LLM) in an environment (ChatGPT UI), reinforment learning is a learning technique to maximize rewards (how good are its generated responses?).

The first step of reinforcement learning is supervised finetuning (SFT), which adapts a pretrained LLM to a specific task. It uses labelled examples to train a model and each prompt-response pair should exemplify desired behavior. While SFT does not require as much data as pretraining, collecting high quality, labeling data can be quite expensive.

The next step is training a reward model, which produces a score for responses generated by the LLM. Typically, there are multiple responses sampled for each prompt, which human labellers indicate their preference for by scoring them. The orignal prompt, generated response, and its corresponding score is then used to finetune the reward model. The challenges of training a reward model include collecting scores from human labellers and having the reward model generalize to training data that is out of distribution.

The last step is to futher finetune our LLM by using the outputs of the reward model as a reward. Each prompt and response generated by the LLM is inputted into the reward model and it calculates a reward for the response. The reward is then used to update the LLM's parameters via a stable policy gradient.

## Repository Structure
```
├── notebooks
|    ├── 00-sft.ipynb <- Data filtering and supervised finetuning
│    └── 01-ppo.ipynb <- Reward modeling and reinforcement learning
|
├── scripts
|     └── tmarco.py
|
├── .gitignore
|
├── README.md <- You are here
|
└── requirements.txt
```

## References
[Understanding Reinforcement Learning From Human Feedback (RHLF): Part 1](https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx#train-policy-using-learned-reward)


